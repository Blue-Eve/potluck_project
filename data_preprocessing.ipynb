{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\", download_dir=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the recipe-data\n",
    "- drop columns\n",
    "- split string columns (ingredients, steps,..) to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = (os.getcwd())+\"/data/RAW_recipes.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(f'{col} has type {type(df[col][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns we don't need\n",
    "\n",
    "df.drop(columns=['contributor_id', 'submitted'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the list columns into a list\n",
    "\n",
    "for col in ['tags', 'nutrition', 'ingredients']:\n",
    "    df[col] = df[col].apply(lambda x: x.strip('[]').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split steps column on ', to split only after the complete step\n",
    "\n",
    "df['steps'] = df['steps'].apply(lambda x: x.strip('[]').split(\"',\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New column for ingredients to search\n",
    "\n",
    "1. lowercase\n",
    "2. remove numbers\n",
    "3. remove spaces\n",
    "4. remove special chars\n",
    "5. lemmatize\n",
    "6. join words with underscores\n",
    "\n",
    "Ingredients:\n",
    "\n",
    "1. capitalize\n",
    "2. connect to string with /n to be displayed correctly\n",
    "\n",
    "Tags, steps:\n",
    "1. remove extra '\n",
    "2. remove spaces\n",
    "3. capitalize first letter\n",
    "4. - for steps - add step number & connect to string with /n to be displayed correctly\n",
    "\n",
    "Nutrition\n",
    "1. change to float\n",
    "\n",
    "Name:\n",
    "1. capitalize first letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning functions\n",
    "\n",
    "def remove_num(text):\n",
    "    return ''.join(char for char in text if not char.isdigit())\n",
    "\n",
    "def remove_punctuation(ingredients):\n",
    "    cleaned_list = []\n",
    "    for word in ingredients:\n",
    "        for punctuation in string.punctuation:\n",
    "            word = word.replace(punctuation, '')\n",
    "        cleaned_list.append(word)\n",
    "    return cleaned_list\n",
    "\n",
    "def lemmatize_ingredients(ingredients):\n",
    "    lemmatized = []\n",
    "    for ingredient in ingredients:\n",
    "        ingredient_words = ingredient.split()\n",
    "        ingredient_lemmatized = [WordNetLemmatizer().lemmatize(word, pos = \"n\") for word in ingredient_words]\n",
    "        lemmatized.append('_'.join(ingredient_lemmatized))\n",
    "    return lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full preproc function \n",
    "\n",
    "def preproc_ingredients(user_input):\n",
    "    #preproces input same way as df\n",
    "\n",
    "    #1.lowercase\n",
    "    user_input = [i.lower() for i in user_input]\n",
    "\n",
    "    #2. remove numbers\n",
    "\n",
    "    user_input = [remove_num(i) for i in user_input]\n",
    "    \n",
    "    #3. remove spaces\n",
    "\n",
    "    user_input =  [i.strip() for i in user_input]\n",
    "\n",
    "    #4. remove special chars\n",
    "\n",
    "    user_input = remove_punctuation(user_input)\n",
    "\n",
    "    #5. lemmatize and join words with underscores\n",
    "\n",
    "    user_input = lemmatize_ingredients(user_input)\n",
    "\n",
    "    #6. change to set\n",
    "\n",
    "    user_input = set(user_input)\n",
    "    \n",
    "    return user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['search_ingredients'] = df['ingredients'].apply(preproc_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean tags and steps\n",
    "\n",
    "df['ingredients'] = df['ingredients'].apply(lambda x: [i.strip().strip(\"'\").capitalize() for i in x])\n",
    "df['tags'] = df['tags'].apply(lambda x: [i.strip().strip(\"'\").capitalize() for i in x])\n",
    "df['steps'] = df['steps'].apply(lambda x: [i.strip().strip(\"'\").capitalize() for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = df['name'].str.capitalize()\n",
    "df['description'] = df['description'].str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change ingredients and steps to presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_line(input_list):\n",
    "    result= \"\"\n",
    "    for element in input_list:\n",
    "        result += f'{element} \\n'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn to enumerate steps\n",
    "\n",
    "def enumerate_steps(input_list):\n",
    "    steps_string = []\n",
    "    for x, y in enumerate(input_list):\n",
    "        steps_string.append(f\"{x+1}. {y}\")\n",
    "    return steps_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate steps on separated lines\n",
    "\n",
    "df['steps'] = df['steps'].apply(enumerate_steps).apply(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'list' ingredients on separated lines\n",
    "\n",
    "df['ingredients'] = df['ingredients'].apply(lambda x: ['- '+ i for i in x]).apply(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean nutrition\n",
    "\n",
    "df['nutrition'] = df['nutrition'].apply(lambda x: [float(i) for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Reviews\n",
    "\n",
    "1. import reviews\n",
    "2. get avg review per recipe\n",
    "3. merge the reviews with the recipe df\n",
    "4. sort by avg review score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import reviews\n",
    "\n",
    "review_path = (os.getcwd())+\"/data/RAW_interactions.csv\"\n",
    "review_df = pd.read_csv(review_path)\n",
    "\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get avg rating & rename columns\n",
    "\n",
    "avg_review_df = review_df[['recipe_id', 'rating']].groupby('recipe_id', as_index=False).mean().round(2)\n",
    "\n",
    "avg_review_df.rename(columns = {'recipe_id': 'id', 'rating':'avg_rating'}, inplace=True)\n",
    "\n",
    "avg_review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge reviews with the recipe df\n",
    "\n",
    "df = df.merge(avg_review_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['avg_rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing recipes for \"hot boiling water\"\n",
    "df = df.drop(df[df.name == \"Brining solution for poultry and meat\"].index)\n",
    "df = df.drop(df[df.name == \"Salted boiling water   what does it mean\"].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save cleaned recipes as local pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df ready! -> save to local pkl\n",
    "saved_pkl_path = os.getcwd()+\"/data/clean_df.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(saved_pkl_path)\n",
    "del df\n",
    "del review_df\n",
    "del avg_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_df = pd.read_pickle(saved_pkl_path)\n",
    "pkl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Food2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to use a pretrained \"Food2Vec\" model - a Word2Vec model trained on recipes/ingredients from github.com/ChantalMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pretrained model\n",
    "\n",
    "#download the data from github\n",
    "#!wget https://github.com/ChantalMP/Exploiting-Food-Embeddings-for-Ingredient-Substitution/releases/download/0.1/food2vec_models.zip\n",
    "\n",
    "#unzip it and save in data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#load the model\n",
    "model_path = os.getcwd()+'/data/model.bin'\n",
    "model = Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with ingredients missing in pretrained Word2Vec space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring dataset ingredients and their embedding in the pretrained model\n",
    "\n",
    "missing_ingredients = []\n",
    "\n",
    "all_ingredients = pkl_df['search_ingredients'].explode().value_counts().index\n",
    "\n",
    "for ingredient in all_ingredients:\n",
    "    try:\n",
    "        model.wv[ingredient]\n",
    "    except:\n",
    "        missing_ingredients.append(ingredient)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of missing ingredients\n",
    "\n",
    "len(missing_ingredients)/len(all_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the missing ingredients contain more than one word\n",
    "\n",
    "len([i for i in missing_ingredients if '_' in i])/len(missing_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all ingredients contain more words -> try searching for reduced ingredient name\n",
    "# -> removing from the front, since lot of the ingredients begin with adjectives\n",
    "\n",
    "ingredients_dict = {}\n",
    "not_existent_ingredients = []\n",
    "\n",
    "for ingredient in missing_ingredients:\n",
    "    search_ingredient = ingredient\n",
    "    in_space = False\n",
    "    while not in_space:\n",
    "        try:\n",
    "            model.wv[search_ingredient]\n",
    "            ingredients_dict[ingredient] = search_ingredient\n",
    "            in_space = True\n",
    "        except KeyError:\n",
    "            try:\n",
    "                search_ingredient = search_ingredient.split(\"_\", maxsplit=1)[1]\n",
    "            except:\n",
    "                not_existent_ingredients.append(ingredient)\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of still missing ingredients\n",
    "\n",
    "len(not_existent_ingredients)/len(all_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column containing only ingredients from the space\n",
    "\n",
    "def move_ingredients_to_space(ingredients):\n",
    "    ingredients = ingredients\n",
    "    in_space_ingredients = set()\n",
    "    for ingredient in ingredients:\n",
    "        in_space_ingredients.add(ingredients_dict.get(ingredient, ingredient))\n",
    "    return in_space_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_df['search_in_space_ingredients'] = pkl_df['search_ingredients'].apply(move_ingredients_to_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the new df\n",
    "\n",
    "pkl_df.to_pickle(saved_pkl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching recipes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for recipe!\n",
    "\n",
    "1. load the df\n",
    "2. load the pretrained model\n",
    "3. get user input as a list\n",
    "4. preproces input same way as df\n",
    "5. extend user input by the nearest ingredients found in the model\n",
    "6. search all \"subset recipes\" of the extended input in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user input -> fix for now (taking ingredients from the first recipe, making sure a match exists)\n",
    "\n",
    "user_input = ['winter squash',\n",
    " 'mexican seasoning',\n",
    " 'mixed spice',\n",
    " 'honey',\n",
    " 'butter',\n",
    " 'olive oil',\n",
    " 'salt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend user input by the k most similar ingredients found in the given model\n",
    "\n",
    "\n",
    "def extend_input(user_input, model, k):\n",
    "    extended_input = set()\n",
    "    for ingredient in user_input:\n",
    "        extended_input.add(ingredient)\n",
    "        search_ingredient = ingredient\n",
    "        found = False\n",
    "        while not found:\n",
    "            try:\n",
    "                k_nearest_ingredients = model.wv.most_similar(search_ingredient, topn=k)\n",
    "                extended_input.add(search_ingredient)\n",
    "                extended_input = extended_input.union({i[0] for i in k_nearest_ingredients})\n",
    "                found = True\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    search_ingredient = search_ingredient.split(\"_\", maxsplit=1)[1]\n",
    "                except:\n",
    "                    break     \n",
    "    return extended_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search all \"subset recipes\" of the extended input in the df\n",
    "\n",
    "def recipe_search(model, df, user_input, k):\n",
    "    #preprocess\n",
    "    preprocced_input = preproc_ingredients(user_input)\n",
    "    \n",
    "    if k == 0:\n",
    "        #search without extending\n",
    "        recipe_df = df[df['search_ingredients'].apply(lambda x: x.issubset(preprocced_input))]     \n",
    "    else:\n",
    "        #extend\n",
    "        k_input = extend_input(preprocced_input, model, k)\n",
    "        recipe_df = df[df['search_in_space_ingredients'].apply(lambda x: x.issubset(k_input))]\n",
    "        \n",
    "    recipe_df['input_matching_rate'] = recipe_df['search_ingredients'].apply(lambda x: len(x.intersection(preprocced_input)))/len(preprocced_input)   \n",
    "    return recipe_df.sort_values(['input_matching_rate', 'avg_rating'], ascending=[False, False]).iloc[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_search(model, pkl_df, user_input,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
